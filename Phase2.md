# **Phase 2: The Quantum Leap â€“ From a Stable Base to a Blazing-Fast Engine**

**Prerequisite:** You have a stable, multi-worker environment running via `docker-compose` (from our revised Step 1.6), and your asynchronous, token-based API is in place.

---
#### **Step 2.1: Implement the Isolate Sandbox Engine**

* **Goal:** To completely replace the slow Docker execution with the near-instantaneous Isolate sandbox, slashing your processing time by over 90%.
* **Why It's a Game-Changer:** This is the single most important performance optimization you will make. It directly attacks the **provisioning latency** that is the source of your 8-second execution time. By switching to Isolate, which has a startup time of 1-2 milliseconds, you make the cost of creating a sandbox negligible.
* **Detailed Changes (What to Do):**
    1.  **Install Isolate on Ubuntu:** `git clone` the official Isolate repository, run `make`, and then `sudo make install`. This makes the `isolate` command available system-wide.
    2.  **Create the Sandbox Module:** Create a new directory `src/sandbox/` containing `mod.rs` and `isolate.rs`.
    3.  **Write the Isolate Wrapper (`src/sandbox/isolate.rs`):**
        * Create a struct, e.g., `IsolateSandbox`.
        * Implement `compile()` and `run()` methods that use Rust's `std::process::Command` to call the `isolate` binary.
        * Your wrapper must manage the entire lifecycle: creating a temporary sandbox directory with `isolate --init`, securely copying files in, running the command with strict resource limits (`--time`, `--mem`, `--fsize`, `--processes`), capturing `stdout` and `stderr`, and cleaning up the sandbox with `isolate --cleanup`.
    4.  **Integrate into the Executor (`src/executor.rs`):**
        * Modify `execute_batch`. Rip out all the logic that calls `get_container`.
        * Instead, for each test case, it will now call your new `IsolateSandbox::run()` method.
        * For compiled languages, it will first call `IsolateSandbox::compile()` once, cache the resulting binary in memory (or Redis), and then use that binary for all subsequent `run` calls in the batch.
* **Implementation Flow:**
    A worker picks up a batch job for a C++ program with 20 test cases. It calls `IsolateSandbox::compile()`. This function creates a sandbox, runs `g++` inside it, and returns the compiled `main` binary. The worker then loops 20 times. In each loop, it calls `IsolateSandbox::run()`, passing in the compiled binary and one of the test case's `stdin`. Because creating and running an Isolate sandbox is so fast, all 20 test cases can be executed in parallel (using `tokio::spawn` and `join_all`) in a fraction of a second.
* **Expected Progress:** This is the breakthrough moment. Your 8-second execution time for a 20-test-case batch will plummet to **under 1 second**. You will have successfully built an execution core that is fundamentally faster than Judge0's.

---
#### **Step 2.2: Implement the "Elastic Reservoir" with a Semaphore**

* **Goal:** To safely manage the concurrency of your new, super-fast Isolate sandboxes, allowing the system to handle massive bursts without crashing.
* **Why It's a Game-Changer:** This makes your engine robust. It prevents a "thundering herd" of 10,000 requests from overwhelming your server's CPU and process limits, ensuring stability under extreme load. It's the professional way to manage high throughput.
* **Detailed Changes (What to Do):**
    1.  **Rename ENV Var:** In your `.env` file, rename `MAX_CONCURRENT_CONTAINERS` to `MAX_CONCURRENT_SANDBOXES`. A value like `500` is a good start.
    2.  **Initialize Semaphore (`src/setup.rs`):** In `initialize_executor`, ensure your `CodeExecutor` struct's `semaphore` is initialized with this new value: `semaphore: Arc::new(Semaphore::new(max_sandboxes))`.
    3.  **Enforce the Limit (`src/executor.rs`):** At the very beginning of your `execute_batch` function, acquire a permit: `let _permit = self.semaphore.acquire().await?;`. The permit will be automatically released when the function ends.
    4.  **Remove All Container Pool Logic:** This is a crucial simplification. You can now **delete the entire `container_pool` field** from the `CodeExecutor` struct. You can also **delete the entire `src/container_management/` directory**. It is now obsolete. The concept of "pooling" is now handled entirely by the semaphore and the millisecond-fast, on-demand creation of Isolate sandboxes.
* **Implementation Flow:**
    A burst of 1000 requests hits your API. Your workers pick up the jobs. Each worker's call to `execute_batch` will first wait for the semaphore. The first 500 workers will get a permit instantly and start creating Isolate sandboxes in parallel. The 501st worker will wait asynchronously and harmlessly until one of the first 500 finishes and releases its permit.
* **Expected Progress:** Your system is now incredibly simple and incredibly scalable on a single node. You have removed a huge amount of complex code (the entire pooling logic) and replaced it with a single, elegant semaphore. The engine can now safely max out your machine's resources to achieve the highest possible throughput without risk of crashing.

---
#### **Step 2.3: Upgrade to a Professional Task Queue (Broccoli)**

* **Goal:** To replace your simple Redis-list-based queue with a production-grade, reliable task queue library.
* **Why It's a Game-Changer:** Your workers are now extremely fast. The next potential point of failure is the queue itself. A simple Redis list doesn't handle failures, retries, or complex routing. Upgrading to a library like Broccoli makes your system's "nervous system" as robust as its "muscles" (the Isolate workers). This is critical for production reliability.
* **New Tools Introduced:**
    * `broccoli`: A Rust-native, distributed task queue library that uses Redis as a backend.
* **Detailed Changes (What to Do):**
    1.  **Add Dependency:** Add `broccoli = "0.5"` to your `Cargo.toml`.
    2.  **Refactor Queue Logic:**
        * You will remove most of the custom logic from `src/queue_management/manager.rs`.
        * Instead, in your API controller (`executionControllers.rs`), you will initialize a Broccoli `Client`. When a request comes in, you will simply call `client.enqueue("execution_queue", task).await`.
        * In your `main.rs`, in the `worker` mode block, you will create a Broccoli `Worker` that consumes from `"execution_queue"`. You will register a function (e.g., `handle_execution_job`) that contains the logic currently in `executor.rs`.
    3.  **Configure Retries:** Configure the Broccoli worker to automatically retry a job up to 3 times if it fails with a transient error.
* **Implementation Flow:**
    A job is enqueued by the API server. A worker picks it up and begins execution. Suddenly, the worker process crashes due to a rare bug. With your old queue, the job might be lost. With Broccoli, the job's state is tracked in Redis. After a timeout, Broccoli sees that the job was never completed and automatically re-enqueues it. Another healthy worker then picks it up and successfully completes it.
* **Expected Progress:** Your system is now **fault-tolerant**. Jobs are never lost. You have achieved a level of reliability that is essential for a production service. You can now confidently scale your workers across multiple machines, and the Broccoli queue will distribute the work among them seamlessly.
