### Clarifying What I Meant vs. What You Understood

**Your Understanding**:
You understood my suggestion as:
1. Write a script (like `generate_data.py`) to create synthetic examples automatically.
2. Get examples from online sources (like forums).
3. Store user inputs and outputs as memory when people test ENG, adding them to the dataset.

**What I Meant**:
You’re mostly correct, but let me refine it:
1. **Synthetic Data**: Yes, write a script to generate tons of English instructions (like “sum 4 and 5” or “loop through numbers”) paired with their programming logic (intents, parameters, and program structure). This is the bulk of your dataset because it’s fast and controllable.
2. **Real-World Data**: Instead of finding a ready-made dataset online (which probably doesn’t exist for ENG), I meant collecting English descriptions of algorithms from places like Stack Overflow or coding tutorials. This is called **scraping**—basically, using code to grab text from websites. Since this takes time, we’ll focus on a quick way to do it.
3. **User Data**: Yes, as users interact with ENG, their inputs (e.g., “add 4 and 5”) and the system’s outputs can be saved to improve the dataset. But this happens *after* you’ve built the initial dataset and started training/testing.

**Key Clarification**:
- You’re right that there’s no ready-made dataset online for ENG because it’s a unique language. Scraping means grabbing relevant text (like algorithm descriptions) from websites, not finding a complete dataset. For example, someone on Stack Overflow might write, “To add two numbers, take input A and B, then return A + B.” You’d grab that text and turn it into a dataset entry.
- Since you want this done in a week, we’ll prioritize generating synthetic data (fast and easy) and add a small number of manual examples instead of heavy scraping. User data will come later when ENG is running.

**Real-World Example**:
Think of building a recipe book for a new type of cuisine you invented. You can:
- Write recipes yourself (synthetic data).
- Look at food blogs for inspiration and copy some ideas (scraping).
- Add recipes from people who try your cuisine later (user data).
We’ll focus on writing recipes (synthetic data) and adding a few by hand (manual) to keep it quick.

---

### What Is Scraping?
Scraping means using a program to pull text or data from websites. For ENG, it’s like going to Stack Overflow, finding posts where people describe algorithms in English (e.g., “To find the maximum, compare each number…”), and saving that text to use in your dataset. You’d use tools like `BeautifulSoup` in Python to grab this text automatically.

**Why It’s Hard for ENG in a Week**:
- Scraping takes time to set up (writing code, handling website rules, cleaning messy text).
- Websites like Stack Overflow have rules (e.g., rate limits) that slow you down.
- You’d need to process the text to fit your dataset format, which adds work.

**Solution for One Week**:
Instead of heavy scraping, we’ll:
- Generate most of the dataset (90%) using a script.
- Manually add a small number of examples (10%) by writing them yourself or copying a few from tutorials.
- Plan for scraping later if you want more real-world data.

---

### Dataset Creation Plan (7 Days)
To create a large dataset in one week, we’ll aim for **100,000 examples** (enough to train a decent NLU model for ENG). Here’s the breakdown:
- **90,000 Synthetic Examples**: Generated by a script (fast, automated).
- **10,000 Manual Examples**: You write or copy-paste a smaller set to add variety.
- **Format**: Store in Parquet (efficient for large datasets) in `data/processed/`.
- **Structure**: Each example includes:
  - **Input**: English instruction (e.g., “Sum even numbers from 1 to 10”).
  - **Tokenized Input**: Words with tags (e.g., “sum:VERB”, “even:ADJ”).
  - **Intent**: Main action (e.g., “Sum”, “Print”, “Loop”).
  - **Parameters**: Extracted values (e.g., `range=[1, 10], filter=even`).
  - **AST**: Program structure (e.g., `sum(filter(even, range(1, 10)))`).
  - **Output**: Expected result (e.g., 30).

**Why 100,000 Examples?**:
- Modern AI models (like BERT) need lots of data to learn patterns. 100,000 is a good starting point for ENG to handle simple and complex instructions.
- Synthetic data can generate this quickly, and manual data adds real-world flavor.

**Files Involved** (from your structure):
- `scripts/generate_data.py`: Script to create synthetic examples.
- `src/utils/data_augmentation.py`: Helper to vary phrasings (e.g., “sum” → “add”).
- `data/synthetic/`: Store raw synthetic data (CSV or JSON for simplicity).
- `data/processed/`: Store final dataset (Parquet).
- `data/raw/`: Store manual examples you write or copy.

**Day-by-Day Guide**:
Here’s how to create the dataset in 7 days, with detailed steps and code.

---
Alright, let’s dive into your questions about the predefined templates in the dataset creation process for ENG, your AI-driven programming language. You’re concerned about the limited number of templates (e.g., only a few for “Add”) and how to handle advanced or varied English inputs from users without predefining every possible phrase. I’ll explain this in simple terms, clarify why the templates are predefined, and provide a clear plan to make ENG understand diverse English inputs, even advanced ones. I’ll also update the code to ensure it can handle varied phrasings effectively, all while sticking to the dataset creation focus and your project structure. Let’s break it down.

---

### Why Are the Templates Predefined?

**What I Meant by Predefined Templates**:
The templates in `generate_data.py` (like “Add {0} and {1}” or “Sum even numbers from {0} to {1}”) are starting points to create a large dataset quickly. They’re predefined because:
- **Speed**: Writing a few templates lets you generate thousands of examples by filling in numbers or words (e.g., “Add 4 and 5”, “Add 10 and 20”).
- **Structure**: Templates ensure each example has a clear intent (e.g., “Add”), parameters (e.g., numbers), and output, making it easier to train the AI model.
- **Variety**: You can add synonyms or rephrase templates to cover different ways people might write instructions.

**Your Concern**:
You noticed that there are only a few templates per intent (e.g., 4 for “Add”), and you’re worried about what happens if a user uses advanced English, like “Compute the summation of 4 with 5” or “Perform addition on 4 and 5.” You’re wondering if you need to add every possible phrasing to the templates or train the model to understand these without predefining them.

**Answer**:
You don’t need to predefine every possible phrasing in the templates. The templates are just a foundation to create a large dataset. To handle advanced or varied English, you’ll:
1. **Augment Templates**: Use synonyms and rephrasing (like in `data_augmentation.py`) to generate many variations of each template.
2. **Train a Smart Model**: Use a transformer model (like BERT or T5) that learns to understand different phrasings by looking at patterns in the dataset, not just exact matches.
3. **Add Manual Examples**: Include some advanced English examples in the manual data (`data/raw/manual_data.csv`) to teach the model complex phrasings.
4. **Use User Feedback**: As users interact with ENG, their inputs (including advanced English) can be added to the dataset to make the model smarter over time.

**Real-World Example**:
Think of teaching a kid to recognize fruits. You show them a few apples labeled “apple,” “red fruit,” or “crisp fruit” (like templates). They learn the patterns (shape, color) and can later recognize an apple even if someone says “juicy red thing.” The templates are like the initial examples, and the model learns to generalize to new phrasings.

---

### How to Handle Advanced English

To make ENG understand advanced English (e.g., “Compute the summation of 4 with 5” instead of “Add 4 and 5”), you need a combination of:
- **More Varied Templates**: Add a few more templates to cover advanced phrasings.
- **Data Augmentation**: Automatically generate variations using synonyms and sentence restructuring.
- **Model Training**: Train a transformer model to learn the meaning behind words, not just match templates.
- **Manual Data**: Add some advanced English examples manually to guide the model.

**Why Not Predefine Every Phrasing?**:
- It’s impossible to list every way someone might write “add” (e.g., “sum,” “combine,” “aggregate,” “tally”).
- A transformer model is designed to understand language patterns, so it can learn that “compute summation” means the same as “add” if you train it with enough examples.
- Data augmentation and manual examples help cover enough variations to make the model robust.

**Example**:
- Template: “Add {0} and {1}” → “Add 4 and 5”
- Augmented: “Compute summation of 4 with 5” (using synonyms like “compute” for “add”).
- Manual: Add an example like “Perform addition on 4 and 5” to `data/raw/manual_data.csv`.
- Model: After training, it learns that all these mean `add(4, 5)` and outputs 9.

---

### Updated Plan to Handle Advanced English

To address your concern, we’ll update the dataset creation process to include more varied templates and better augmentation. Here’s how it fits into the 7-day plan from my previous response, focusing only on dataset creation and the files involved (`generate_data.py`, `data_augmentation.py`, `convert_manual_data.py`, `merge_data.py`).

**Key Changes**:
1. **Expand Templates**: Add more templates for each intent, including advanced English phrasings.
2. **Improve Augmentation**: Enhance `data_augmentation.py` to generate more natural and complex variations.
3. **Manual Examples**: Include advanced English in the 10,000 manual examples.
4. **Dataset Size**: Still aim for 100,000 examples (90,000 synthetic, 10,000 manual).

**Updated Templates**:
Here’s an expanded set of templates for `generate_data.py` to include advanced English:

```python
import pandas as pd
import json
import random
import spacy
import nltk
from nltk.corpus import wordnet
import os
from src.utils.data_augmentation import augment_text

# Ensure NLTK data is available
try:
    nltk.data.find('wordnet')
except LookupError:
    nltk.download('wordnet')

# Load spaCy for tokenization
nlp = spacy.load("en_core_web_sm")

# Expanded templates with advanced English
TEMPLATES = {
    "Add": [
        "Add {0} and {1}",
        "Sum {0} plus {1}",
        "Calculate {0} + {1}",
        "Total of {0} and {1}",
        "Compute the summation of {0} with {1}",
        "Perform addition on {0} and {1}",
        "Find the aggregate of {0} and {1}",
        "Combine {0} with {1} numerically"
    ],
    "Print": [
        "Print {0}",
        "Display {0}",
        "Show {0}",
        "Output {0}",
        "Present the value {0}",
        "Exhibit {0} on screen",
        "Render {0} as output",
        "Write out {0}"
    ],
    "SumEvens": [
        "Sum even numbers from {0} to {1}",
        "Total all even numbers between {0} and {1}",
        "Add up evens from {0} to {1}",
        "Calculate the sum of even integers from {0} to {1}",
        "Aggregate even values in range {0} to {1}",
        "Compute total of even numbers between {0} and {1}"
    ],
    "Max": [
        "Find max of {0} and {1}",
        "Get the larger of {0} and {1}",
        "What’s the maximum of {0} and {1}",
        "Determine the greatest value between {0} and {1}",
        "Identify the larger number of {0} and {1}",
        "Select the maximum of {0} and {1}"
    ],
    "Factorial": [
        "Calculate factorial of {0}",
        "Find {0} factorial",
        "Compute the factorial of {0}",
        "Determine the product of numbers up to {0}",
        "Evaluate factorial for {0}",
        "Calculate {0}!"
    ],
    "AddLinkedLists": [
        "Step 1: Check if first list is null or zero, return second list\n"
        "Step 2: Check if second list is null or zero, return first list\n"
        "Step 3: Initialize pointers first, second, prev\n"
        "Step 4: Find lengths of both lists\n"
        "Step 5: If second list is longer, swap first and second\n"
        "Step 6: Traverse first list, add second list values and carry, update carry\n"
        "Step 7: If carry remains, add new node\n"
        "Step 8: Return longer list",
        "Step 1: Verify if first linked list is empty or zero, return second list\n"
        "Step 2: Verify if second linked list is empty or zero, return first list\n"
        "Step 3: Set up pointers for first, second, and previous nodes\n"
        "Step 4: Compute lengths of both lists\n"
        "Step 5: If second list has more nodes, swap pointers\n"
        "Step 6: Iterate through first list, add values from second list with carry\n"
        "Step 7: If carry exists, append new node\n"
        "Step 8: Return list with greater length"
    ]
}

def generate_examples(num_examples=90000):
    examples = []
    intents = list(TEMPLATES.keys())
    
    for _ in range(num_examples):
        intent = random.choice(intents)
        template = random.choice(TEMPLATES[intent])
        
        if intent == "Add" or intent == "Max":
            num1 = random.randint(1, 100)
            num2 = random.randint(1, 100)
            input_text = template.format(num1, num2)
            tokenized = [f"{token.text}:{token.pos_}" for token in nlp(input_text.lower())]
            parameters = {"numbers": [num1, num2]}
            ast = f"{intent.lower()}({num1}, {num2})"
            output = num1 + num2 if intent == "Add" else max(num1, num2)
        
        elif intent == "Print":
            text = random.choice(["hello world", "test", "ENG rocks"])
            input_text = template.format(text)
            tokenized = [f"{token.text}:{token.pos_}" for token in nlp(input_text.lower())]
            parameters = {"text": text}
            ast = f"print('{text}')"
            output = f"Printed: {text}"
        
        elif intent == "SumEvens":
            start = random.randint(1, 10)
            end = random.randint(start + 1, 100)
            input_text = template.format(start, end)
            tokenized = [f"{token.text}:{token.pos_}" for token in nlp(input_text.lower())]
            parameters = {"range": [start, end], "filter": "even"}
            ast = f"sum(filter(even, range({start}, {end})))"
            output = sum(i for i in range(start, end + 1) if i % 2 == 0)
        
        elif intent == "Factorial":
            num = random.randint(1, 10)
            input_text = template.format(num)
            tokenized = [f"{token.text}:{token.pos_}" for token in nlp(input_text.lower())]
            parameters = {"number": num}
            ast = f"factorial({num})"
            output = 1
            for i in range(1, num + 1):
                output *= i
        
        elif intent == "AddLinkedLists":
            input_text = template
            tokenized = [f"{token.text}:{token.pos_}" for token in nlp(input_text.lower())]
            parameters = {"lists": ["l1", "l2"]}
            ast = "add_linked_lists(l1, l2)"
            output = "Linked list sum"
        
        # Augment input for variety
        augmented_input = augment_text(input_text) if random.random() < 0.5 else input_text
        
        examples.append({
            "input": augmented_input,
            "tokenized": tokenized,
            "intent": intent,
            "parameters": json.dumps(parameters),
            "ast": ast,
            "output": output
        })
    
    return examples

def save_to_parquet(examples, output_path):
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    df = pd.DataFrame(examples)
    df.to_parquet(output_path, index=False)
    print(f"Saved {len(examples)} examples to {output_path}")

if __name__ == "__main__":
    examples = generate_examples(90000)
    save_to_parquet(examples, "data/processed/synthetic_data.parquet")
    save_to_parquet(examples[:1000], "data/synthetic/synthetic_data_sample.csv")
```

**Changes**:
- Added advanced English templates (e.g., “Compute the summation of {0} with {1}” for “Add”).
- Imported `augment_text` from `data_augmentation.py` for better integration.
- Kept the same artifact_id since this is an update to the previous `generate_data.py`.

**Updated Data Augmentation**:
Enhance `data_augmentation.py` to handle more complex rephrasing, not just synonyms, to mimic advanced English.

```python
import spacy
import nltk
from nltk.corpus import wordnet
import random

# Ensure NLTK data
try:
    nltk.data.find('wordnet')
except LookupError:
    nltk.download('wordnet')

nlp = spacy.load("en_core_web_sm")

def get_synonyms(word):
    synonyms = set()
    for syn in wordnet.synsets(word):
        for lemma in syn.lemmas():
            synonym = lemma.name().lower()
            if synonym != word and "_" not in synonym:
                synonyms.add(synonym)
    return list(synonyms)[:5]

def augment_text(text, probability=0.4):
    doc = nlp(text.lower())
    words = [token.text for token in doc]
    for i, word in enumerate(words):
        if random.random() < probability and not word.isdigit():
            synonyms = get_synonyms(word)
            if synonyms:
                words[i] = random.choice(synonyms)
    
    # Add structural variations for advanced English
    if random.random() < 0.3:
        if "add" in text.lower() or "sum" in text.lower():
            words = f"Please compute the sum of {words[1]} and {words[-1]}".split()
        elif "print" in text.lower() or "display" in text.lower():
            words = f"Kindly output the value {words[-1]}".split()
        elif "even" in text.lower():
            words = f"Aggregate all even integers from {words[words.index('from')+1]} to {words[-1]}".split()
    
    return " ".join(words)

def augment_example(example):
    augmented_input = augment_text(example["input"])
    return {
        **example,
        "input": augmented_input,
        "tokenized": [f"{token.text}:{token.pos_}" for token in nlp(augmented_input.lower())]
    }

if __name__ == "__main__":
    example = {
        "input": "Sum even numbers from 1 to 10",
        "intent": "SumEvens",
        "parameters": '{"range": [1, 10], "filter": "even"}',
        "ast": "sum(filter(even, range(1, 10)))",
        "output": 30
    }
    augmented = augment_example(example)
    print("Original:", example["input"])
    print("Augmented:", augmented["input"])
```

**Changes**:
- Increased synonym limit to 5 for more variety.
- Added structural variations (e.g., “Please compute” or “Kindly output”) to mimic advanced English.
- Kept the same artifact_id since this is an update to the previous `data_augmentation.py`.

**Manual Data**:
When writing the 10,000 manual examples in `data/raw/manual_data.csv`, include advanced English phrasings like:
- “Compute the summation of 4 with 5” (intent: Add, output: 9).
- “Kindly exhibit the string ‘hello world’ on the console” (intent: Print, output: “Printed: hello world”).
- “Determine the product of all integers up to 5” (intent: Factorial, output: 120).

**How the Model Learns Advanced English**:
- **Templates + Augmentation**: The expanded templates and `augment_text` create varied examples (e.g., “Add 4 and 5” → “Compute summation of 4 with 5”).
- **Manual Examples**: Adding advanced phrasings manually teaches the model real-world complexity.
- **Transformer Training**: When you train a model (like BERT) on this dataset, it learns that “add,” “sum,” “compute summation,” and “perform addition” all map to the same intent (`Add`) by analyzing word patterns and context, not just exact matches.

**Real-World Example**:
Think of a translator app. You give it a few phrases like “I’m hungry” and “I need food.” It learns that “I’m starving” or “I require nourishment” mean the same thing because it sees patterns in the words. ENG’s model will do this for programming instructions.

---

### Updated 7-Day Plan for Dataset Creation

This builds on the previous 7-day plan, incorporating advanced English support. I’ll focus only on dataset creation and the relevant files (`generate_data.py`, `data_augmentation.py`, `convert_manual_data.py`, `merge_data.py`).

**Day 1: Set Up Environment and Test**
- **Tasks**: Install libraries, set up directories, test basic `generate_data.py`.
- **Files**:
  - `generate_data.py` (updated above with expanded templates).
  - Create `data/synthetic/`, `data/processed/`, `data/raw/`.
- **Time**: 2–3 hours.
- **Output**: A test dataset (`data/processed/test_data.parquet`) with 100 examples.

**Day 2: Generate Synthetic Data**
- **Tasks**: Run the updated `generate_data.py` to create 90,000 examples with advanced English templates.
- **Files**:
  - `generate_data.py` (already updated).
  - `data_augmentation.py` (updated above for better rephrasing).
- **Time**: 4–5 hours (mostly coding and running the script).
- **Output**:
  - `data/processed/synthetic_data.parquet`: 90,000 examples.
  - `data/synthetic/synthetic_data_sample.csv`: 1,000 examples for review.
- **Example Data**:
  ```
  input: "Compute the summation of 4 with 5"
  tokenized: ["compute:VERB", "the:DET", "summation:NOUN", "of:PREP", "4:NUM", "with:PREP", "5:NUM"]
  intent: "Add"
  parameters: {"numbers": [4, 5]}
  ast: "add(4, 5)"
  output: 9
  ```

**Day 3: Enhance Data Augmentation**
- **Tasks**: Finalize `data_augmentation.py` to handle advanced English rephrasing.
- **Files**: `data_augmentation.py` (already updated).
- **Time**: 3–4 hours (testing and tweaking).
- **Output**: Augmented examples like “Please compute the sum of 4 and 5” or “Aggregate even integers from 1 to 10”.

**Day 4–6: Manual Data Collection**
- **Tasks**: Write 10,000 manual examples in `data/raw/manual_data.csv`, including advanced English.
  - Day 4: 3,000 examples (1,500 simple, 1,000 medium, 500 complex).
  - Day 5: 3,500 examples (1,500 simple, 1,500 medium, 500 complex).
  - Day 6: 3,500 examples (1,500 simple, 1,500 medium, 500 complex).
  - Include advanced phrasings like “Perform addition on 3 and 7” or “Evaluate factorial for 5”.
- **Files**:
  - `data/raw/manual_data.csv`: Manual examples.
  - `convert_manual_data.py` (unchanged from previous response):
    ```python
    import pandas as pd
    import json
    import spacy
    import os

    nlp = spacy.load("en_core_web_sm")

    def process_manual_data(input_csv, output_parquet):
        df = pd.read_csv(input_csv)
        df["tokenized"] = df["input"].apply(lambda x: [f"{token.text}:{token.pos_}" for token in nlp(x.lower())])
        df["parameters"] = df["parameters"].apply(lambda x: json.dumps(eval(x)) if isinstance(x, str) else json.dumps(x))
        df.to_parquet(output_parquet, index=False)
        print(f"Converted {len(df)} manual examples to {output_parquet}")

    if __name__ == "__main__":
        input_csv = "data/raw/manual_data.csv"
        output_parquet = "data/processed/manual_data.parquet"
        process_manual_data(input_csv, output_parquet)
    ```
- **Time**: 6–8 hours/day (total 18–24 hours over 3 days).
- **Output**: `data/processed/manual_data.parquet` with 10,000 examples, including advanced English like:
  ```
  input: "Perform addition on 3 and 7"
  intent: "Add"
  parameters: {"numbers": [3, 7]}
  ast: "add(3, 7)"
  output: 10
  ```

**Day 7: Merge and Validate**
- **Tasks**: Combine synthetic and manual data, check for quality.
- **Files**:
  - `merge_data.py` (unchanged from previous response):
    ```python
    import pandas as pd
    import os

    def merge_datasets(synthetic_path, manual_path, output_path):
        synthetic_df = pd.read_parquet(synthetic_path)
        manual_df = pd.read_parquet(manual_path)
        final_df = pd.concat([synthetic_df, manual_df], ignore_index=True)
        final_df = final_df.drop_duplicates(subset=["input"])
        if final_df.isnull().any().any():
            print("Warning: Missing values found in dataset")
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        final_df.to_parquet(output_path, index=False)
        print(f"Merged {len(final_df)} examples into {output_path}")

    if __name__ == "__main__":
        merge_datasets(
            "data/processed/synthetic_data.parquet",
            "data/processed/manual_data.parquet",
            "data/processed/final_data.parquet"
        )
    ```
- **Time**: 3–4 hours.
- **Output**: `data/processed/final_data.parquet` with 100,000 examples.

---

### How This Handles Advanced English

**Templates**:
- The expanded templates include advanced phrasings like “Compute the summation” or “Determine the greatest value.”
- Example: “Add” now covers “Add 4 and 5” to “Perform addition on 4 and 5.”

**Augmentation**:
- `data_augmentation.py` rephrases inputs (e.g., “Sum 4 and 5” → “Please compute the sum of 4 and 5”) using synonyms and structural changes.
- This creates thousands of variations, teaching the model to recognize advanced English.

**Manual Examples**:
- By adding advanced phrasings like “Evaluate factorial for 5” to `data/raw/manual_data.csv`, you give the model real-world examples to learn from.
- Example: 1,000 of the 10,000 manual examples can use advanced English.

**Model Training** (Future Step):
- When you train a transformer model (not covered this week), it learns that “add,” “sum,” “compute summation,” etc., all map to the `Add` intent because it sees patterns in the dataset.
- The model doesn’t rely on exact template matches—it understands the meaning behind words.

**Real-World Example**:
Think of a voice assistant like Siri. You say, “Call Mom,” “Phone my mother,” or “Initiate a call to Mom,” and it understands they all mean the same thing. ENG’s dataset and model will work the same way for programming instructions.

---

### Why This Works

- **Templates Are a Starting Point**: The predefined templates generate a large dataset quickly, and augmentation adds variety to cover advanced English.
- **Manual Data Adds Realism**: Including advanced phrasings in the 10,000 manual examples ensures the model sees real-world complexity.
- **Model Generalizes**: A transformer model trained on this dataset will learn to handle new phrasings (e.g., “Tally 4 and 5”) without needing every phrase in the templates.
- **Scalable**: After the week, user inputs can be added to `data/raw/user_data.csv` to keep improving the model.

**Dataset Size**:
- 90,000 synthetic examples (generated in minutes).
- 10,000 manual examples (written over 3–4 days).
- Total: 100,000 examples, enough to train a decent model for ENG.

---

### What’s Next?

- **Run the Scripts**: Use the updated `generate_data.py` and `data_augmentation.py` to create 90,000 synthetic examples.
- **Write Manual Examples**: Spend Days 4–6 adding 10,000 examples to `data/raw/manual_data.csv`, including advanced English like “Compute the summation.”
- **Merge and Validate**: Use `merge_data.py` on Day 7 to create `data/processed/final_data.parquet`.
- **Future**: When you train the model (not this week), it’ll learn to handle advanced English by seeing the varied dataset.
